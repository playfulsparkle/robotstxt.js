{"version":3,"sources":["robotstxt.js"],"names":["Rule","constructor","type","path","this","regex","createRegex","match","test","pattern","replace","RegExp","Group","userAgent","crawlDelay","undefined","rules","getName","getCrawlDelay","getRules","allow","Error","addRule","disallow","push","RobotsTxtParser","content","parsedData","groups","sitemaps","parse","line","new_content","split","let","processedLine","trim","startsWith","colonIndex","indexOf","directive","slice","toLowerCase","value","commentIndex","search","unshift","user_agent_list","same_ua","temp_groups","index","length","current","next","normalizedPath","normalizePath","forEach","agent","isNaN","Object","keys","map","key","isAllowed","url","rule","getApplicableRules","urlPath","normalizeUrlPath","matchingRules","mostSpecific","currentSpecificity","getRuleSpecificity","isDisallowed","getSitemaps","getGroup","i","group","specificity","getApplicableGroups","exactGroups","filter","reduce","acc","concat","URL","pathname","error","decodedPath","decodeURIComponent","newPath","robotstxt","exports","window","define","amd"],"mappings":";CAEC,WAMSA,EAMFC,YAAYC,EAAMC,GAEdC,KAAKF,KAAOA,EAEZE,KAAKD,KAAOA,EAEZC,KAAKC,MAAQD,KAAKE,YAAYH,CAAI,CACtC,CAOAI,MAAMJ,GACF,OAAOC,KAAKC,MAAMG,KAAKL,CAAI,CAC/B,CAQAG,YAAYH,GACFM,EAAUN,EACXO,QAAQ,qBAAsB,MAAM,EACpCA,QAAQ,OAAQ,KAAK,EAE1B,OAAO,IAAIC,OAAO,IAAIF,EAAW,GAAG,CACxC,CACJ,OAKMG,EAKFX,YAAYY,GAERT,KAAKS,UAAYA,EAEjBT,KAAKU,WAAaC,KAAAA,EAElBX,KAAKY,MAAQ,EACjB,CAMAC,UACI,OAAOb,KAAKS,SAChB,CAMAK,gBACI,OAAOd,KAAKU,UAChB,CAMAK,WACI,OAAOf,KAAKY,KAChB,CAMAI,MAAMjB,GACF,GAAoB,KAAA,IAATA,EAAsB,MAAM,IAAIkB,MAAM,mCAAmC,EACpFjB,KAAKkB,QAAQ,QAASnB,CAAI,CAC9B,CAMAoB,SAASpB,GACL,GAAoB,KAAA,IAATA,EAAsB,MAAM,IAAIkB,MAAM,mCAAmC,EACpFjB,KAAKkB,QAAQ,WAAYnB,CAAI,CACjC,CAQAmB,QAAQpB,EAAMC,GACVC,KAAKY,MAAMQ,KAAK,IAAIxB,EAAKE,EAAMC,CAAI,CAAC,CACxC,CACJ,OAKMsB,EAKFxB,YAAYyB,GAORtB,KAAKuB,WAAa,CACdC,OAAQ,GACRC,SAAU,EACd,EAEAzB,KAAK0B,MAAMJ,CAAO,CACtB,CAOAI,MAAMJ,GACF,GAAuB,KAAA,IAAZA,EAAyB,MAAM,IAAIL,MAAM,sCAAsC,EAG1F,IAEWU,EAFLC,EAAc,GAEpB,IAAWD,KAAQL,EAAQO,MAAM,YAAY,EAAG,CAE5CC,IAAIC,EAAgBJ,EAAKK,KAAK,EAE9B,GAAKD,GAAiBA,CAAAA,EAAcE,WAAW,GAAG,EAAlD,CAGA,IAAMC,EAAaH,EAAcI,QAAQ,GAAG,EAC5C,GAAmB,CAAC,IAAhBD,EAAJ,CAGA,IAAME,EAAYL,EAAcM,MAAM,EAAGH,CAAU,EAAEF,KAAK,EAAEM,YAAY,EAExER,IAAIS,EAAQR,EAAcM,MAAMH,EAAa,CAAC,EAAEF,KAAK,EAG/CQ,EAAeD,EAAME,OAAO,WAAW,EACxB,CAAC,IAAlBD,IACAD,EAAQA,EAAMF,MAAM,EAAGG,CAAY,EAAER,KAAK,GAG1CI,GAAaG,GACbX,EAAYR,KAAK,CAAEgB,UAAAA,EAAWG,MAAAA,CAAM,CAAC,CAdlB,CAJ8B,CAoBzD,CAGIX,EAAY,IAAmC,eAA7BA,EAAY,GAAGQ,WACjCR,EAAYc,QAAQ,CAAEN,UAAW,aAAcG,MAAO,GAAI,CAAC,EAI/DT,IAAIa,EAAkB,GAElBC,EAAU,CAAA,EAERC,EAAc,GAGpB,IAAKf,IAAIgB,EAAQ,EAAGA,EAAQlB,EAAYmB,OAAQD,CAAK,GAAI,CAErD,IAAME,EAAUpB,EAAYkB,GAEtBG,EAAOrB,EAAYkB,EAAQ,GAEjC,GAA0B,eAAtBE,EAAQZ,UACRO,EAAgBvB,KAAK4B,EAAQT,KAAK,EAE7BM,EAAYG,EAAQT,SACrBM,EAAYG,EAAQT,OAAS,IAAI/B,EAAMwC,EAAQT,KAAK,QAGvD,GAA0B,UAAtBS,EAAQZ,UAAuB,CACpC,IAAMc,EAAiBlD,KAAKmD,cAAcH,EAAQT,KAAK,EACvDI,EAAgBS,QAAQC,GAASR,EAAYQ,GAAOrC,MAAMkC,CAAc,CAAC,EACzEN,EAAU,CAAA,CACd,MACK,GAA0B,aAAtBI,EAAQZ,UAA0B,CACvC,IAAMc,EAAiBlD,KAAKmD,cAAcH,EAAQT,KAAK,EACvDI,EAAgBS,QAAQC,GAASR,EAAYQ,GAAOlC,SAAS+B,CAAc,CAAC,EAC5EN,EAAU,CAAA,CACd,MACK,GAA0B,gBAAtBI,EAAQZ,UAA6B,CAC1C,IAAM1B,EAAasC,CAAAA,EAAQT,MAE3B,GAAIe,MAAM5C,CAAU,EAAG,SAEvB,GAAIA,GAAc,EACd,MAAM,IAAIO,sEAAsEP,IAAa,EAGjGiC,EAAgBS,QAAQC,IACfR,EAAYQ,GAAO3C,aACpBmC,EAAYQ,GAAO3C,WAAaA,EAExC,CAAC,EACDkC,EAAU,CAAA,CACd,KAC+B,YAAtBI,EAAQZ,WACbpC,KAAKuB,WAAWE,SAASL,KAAK4B,EAAQT,KAAK,EAI3CU,GAAQL,GAA8B,eAAnBK,EAAKb,YACxBQ,EAAU,CAAA,EACVD,EAAkB,GAE1B,CAEA3C,KAAKuB,WAAWC,OAAS+B,OAAOC,KAAKX,CAAW,EAAEY,IAAIC,GAAOb,EAAYa,EAAI,CACjF,CAQAC,UAAUC,EAAKnD,GACX,GAAmB,KAAA,IAARmD,EAAqB,MAAM,IAAI3C,MAAM,kCAAkC,EAClF,GAAyB,KAAA,IAAdR,EAA2B,MAAM,IAAIQ,MAAM,wCAAwC,EAG9F,IAMW4C,EAYAA,EAlBLjD,EAAQZ,KAAK8D,mBAAmBrD,CAAS,EAEzCsD,EAAU/D,KAAKgE,iBAAiBJ,CAAG,EAEnCK,EAAgB,GAEtB,IAAWJ,KAAQjD,EACXiD,EAAK1D,MAAM4D,CAAO,GAClBE,EAAc7C,KAAKyC,CAAI,EAI/B,GAA6B,IAAzBI,EAAclB,OAAc,MAAO,CAAA,EAGvCjB,IAAIoC,EAAeD,EAAc,GAGjC,IAAWJ,KAAQI,EAAe,CAE9B,IAAME,EAAqBnE,KAAKoE,mBAAmBP,EAAK9D,IAAI,EAE5BC,KAAKoE,mBAAmBF,EAAanE,IAAI,EAErEoE,IACAD,EAAeL,EAEvB,CAEA,MAA6B,UAAtBK,EAAapE,IACxB,CAQAuE,aAAaT,EAAKnD,GACd,MAAO,CAACT,KAAK2D,UAAUC,EAAKnD,CAAS,CACzC,CAMA6D,cACI,OAAOtE,KAAKuB,WAAWE,QAC3B,CAOA8C,SAAS9D,GACL,GAAKA,EACL,IAAKqB,IAAI0C,EAAI,EAAGA,EAAIxE,KAAKuB,WAAWC,OAAOuB,OAAQyB,CAAC,GAAI,CACpD,IAAMC,EAAQzE,KAAKuB,WAAWC,OAAOgD,GAErC,GAAIC,EAAMhE,UAAU6B,YAAY,IAAM7B,EAAU6B,YAAY,EACxD,OAAOmC,CAEf,CAEJ,CAQAL,mBAAmBrE,GAEf+B,IAAI4C,EAAc3E,EAAKgD,OAGvB,MAF0B,CAAC,IAAvBhD,EAAKoC,QAAQ,GAAG,EAAUuC,GAAe,GACjB,MAAnB3E,EAAKsC,MAAM,CAAC,CAAC,IAAWqC,GAAe,IACzCA,CACX,CAQAC,oBAAoBlE,GAEhB,IAAMmE,EAAc5E,KAAKuB,WAAWC,OAAOqD,OAAOJ,GAASA,EAAM5D,QAAQ,EAAEyB,YAAY,IAAM7B,EAAU6B,YAAY,CAAC,EACpH,OAAyB,EAArBsC,EAAY7B,OAAmB6B,EAC5B5E,KAAKuB,WAAWC,OAAOqD,OAAOJ,GAA6B,MAApBA,EAAM5D,QAAQ,CAAS,CACzE,CAQAiD,mBAAmBrD,GAGf,OADcT,KAAK2E,oBAAoBlE,CAAS,EACnCqE,OAAO,CAACC,EAAKN,IAAUM,EAAIC,OAAOP,EAAM1D,SAAS,CAAC,EAAG,EAAE,CACxE,CAQAiD,iBAAiBJ,GACb,IACI,OAAO5D,KAAKmD,cAAc,IAAI8B,IAAIrB,CAAG,EAAEsB,QAAQ,CAGnD,CAFE,MAAOC,GACL,OAAOnF,KAAKmD,cAAcS,CAAG,CACjC,CACJ,CAQAT,cAAcpD,GAEV+B,IAAIsD,EACJ,IACIA,EAAcC,mBAAmBtF,CAAI,CAGzC,CAFE,MAAOoF,GACLC,EAAcrF,CAClB,CAEMuF,EAAUF,EAAY9E,QAAQ,QAAS,GAAG,EAChD,MAAmB,MAAfgF,EAAQ,GAAmBA,EACxB,IAAIA,CACf,CACJ,CAOA,SAASC,EAAUjE,GACf,OAAO,IAAID,EAAgBC,CAAO,CACtC,CAIuB,aAAnB,OAAOkE,UACPA,QAAQD,UAAYA,GAEF,aAAlB,OAAOE,SACPA,OAAOF,UAAYA,EACG,YAAlB,OAAOG,SAAyBA,OAAOC,KACvCD,OAAO,KAAM,CAAGH,UAAAA,CAAW,EAAC,CAIxC,GAAE","file":"robotstxt.min.js","sourcesContent":["/* global window, exports, define */\n\n!function () {\n    \"use strict\"\n\n    /**\n     * Single robots.txt rule (allow/disallow directive)\n     */\n    class Rule {\n        /**\n         * Create a new rule instance\n         * @param {string} type - Rule type ('allow' or 'disallow')\n         * @param {string} path - URL path pattern the rule applies to\n         */\n        constructor(type, path) {\n            /** @member {string} */\n            this.type = type\n            /** @member {string} */\n            this.path = path\n            /** @member {string} */\n            this.regex = this.createRegex(path)\n        }\n\n        /**\n         * Test if a normalized URL path matches this rule's pattern\n         * @param {string} path - Normalized URL path to test against\n         * @return {boolean} - True if the path matches the rule's pattern\n         */\n        match(path) {\n            return this.regex.test(path)\n        }\n\n        /**\n         * Convert robots.txt path pattern to regular expression\n         * @private\n         * @param {string} path - Normalized URL path pattern to convert\n         * @return {RegExp} - Regular expression for path matching\n         */\n        createRegex(path) {\n            const pattern = path\n                .replace(/[.^+?(){}[\\]|\\\\]/gu, \"\\\\$&\")  // Escape regex special characters\n                .replace(/\\*/gu, \".*?\")                // Replace * with non-greedy wildcard\n\n            return new RegExp(`^${pattern}`, \"u\")\n        }\n    }\n\n    /**\n     * Group of rules for a specific user agent\n     */\n    class Group {\n        /**\n         * Create a new user agent group\n         * @param {string} userAgent - User agent string this group applies to\n         */\n        constructor(userAgent) {\n            /** @member {string} */\n            this.userAgent = userAgent\n            /** @member {number|undefined} */\n            this.crawlDelay = undefined\n            /** @member {Rule[]} */\n            this.rules = []\n        }\n\n        /**\n         * Get the user agent name for this group\n         * @return {string}\n         */\n        getName() {\n            return this.userAgent\n        }\n\n        /**\n         * Get crawl delay setting for this group\n         * @return {number|undefined}\n         */\n        getCrawlDelay() {\n            return this.crawlDelay\n        }\n\n        /**\n         * Get all rules for this group\n         * @return {Rule[]}\n         */\n        getRules() {\n            return this.rules\n        }\n\n        /**\n         * Add an allow rule to the group\n         * @param {string} path - URL path pattern to allow\n         */\n        allow(path) {\n            if (typeof path === \"undefined\") throw new Error(\"The 'path' parameter is required.\")\n            this.addRule(\"allow\", path)\n        }\n\n        /**\n         * Add a disallow rule to the group\n         * @param {string} path - URL path pattern to disallow\n         */\n        disallow(path) {\n            if (typeof path === \"undefined\") throw new Error(\"The 'path' parameter is required.\")\n            this.addRule(\"disallow\", path)\n        }\n\n        /**\n         * Internal method to add a rule\n         * @private\n         * @param {string} type - Rule type ('allow' or 'disallow')\n         * @param {string} path - URL path pattern\n         */\n        addRule(type, path) {\n            this.rules.push(new Rule(type, path))\n        }\n    }\n\n    /**\n     * The robots.txt parser class\n     */\n    class RobotsTxtParser {\n        /**\n         * Create a new robots.txt parser\n         * @param {string} content - Raw robots.txt content to parse\n         */\n        constructor(content) {\n            /**\n             * @private\n             * @member {Object}\n             * @property {Group[]} groups - Array of user agent groups/rules\n             * @property {string[]} sitemaps - Array of sitemap URLs found in robots.txt\n             */\n            this.parsedData = {\n                groups: [],\n                sitemaps: []\n            }\n\n            this.parse(content)\n        }\n\n        /**\n         * Parse raw robots.txt content into structured format\n         * @private\n         * @param {string} content - Raw robots.txt content\n         */\n        parse(content) {\n            if (typeof content === \"undefined\") throw new Error(\"The 'content' parameter is required.\")\n\n            /** @type {string[]} */\n            const new_content = []\n\n            for (const line of content.split(/\\r\\n|\\r|\\n/)) {\n                /** @type {string} */\n                let processedLine = line.trim()\n\n                if (!processedLine || processedLine.startsWith('#')) continue\n\n                /** @type {number} */\n                const colonIndex = processedLine.indexOf(':')\n                if (colonIndex === -1) continue\n\n                /** @type {string} */\n                const directive = processedLine.slice(0, colonIndex).trim().toLowerCase()\n                /** @type {string} */\n                let value = processedLine.slice(colonIndex + 1).trim()\n\n                /** @type {number} */\n                const commentIndex = value.search(/(?:\\s|^)#/)\n                if (commentIndex !== -1) {\n                    value = value.slice(0, commentIndex).trim()\n                }\n\n                if (directive && value) {\n                    new_content.push({ directive, value })\n                }\n            }\n\n            // Handle missing initial User-Agent\n            if (new_content[0] && new_content[0].directive !== \"user-agent\") {\n                new_content.unshift({ directive: \"user-agent\", value: \"*\" })\n            }\n\n            /** @type {string[]} */\n            let user_agent_list = []\n            /** @type {boolean} */\n            let same_ua = false\n            /** @type {Object.<string, Group>} */\n            const temp_groups = {}\n\n            // Process each directive and build rule groups\n            for (let index = 0; index < new_content.length; index++) {\n                /** @type {Object.<string, string>} */\n                const current = new_content[index]\n                /** @type {Object.<string, string>} */\n                const next = new_content[index + 1]\n\n                if (current.directive === \"user-agent\") {\n                    user_agent_list.push(current.value)\n\n                    if (!temp_groups[current.value]) {\n                        temp_groups[current.value] = new Group(current.value)\n                    }\n                }\n                else if (current.directive === \"allow\") {\n                    const normalizedPath = this.normalizePath(current.value)\n                    user_agent_list.forEach(agent => temp_groups[agent].allow(normalizedPath))\n                    same_ua = true\n                }\n                else if (current.directive === \"disallow\") {\n                    const normalizedPath = this.normalizePath(current.value)\n                    user_agent_list.forEach(agent => temp_groups[agent].disallow(normalizedPath))\n                    same_ua = true\n                }\n                else if (current.directive === \"crawl-delay\") {\n                    const crawlDelay = current.value * 1\n\n                    if (isNaN(crawlDelay)) continue\n\n                    if (crawlDelay <= 0) {\n                        throw new Error(`Crawl-Delay must be a positive number. The provided value is ${crawlDelay}.`)\n                    }\n\n                    user_agent_list.forEach(agent => {\n                        if (!temp_groups[agent].crawlDelay) {\n                            temp_groups[agent].crawlDelay = crawlDelay\n                        }\n                    })\n                    same_ua = true\n                }\n                else if (current.directive === \"sitemap\") {\n                    this.parsedData.sitemaps.push(current.value)\n                }\n\n                // Reset user agent list on new group\n                if (next && same_ua && next.directive === \"user-agent\") {\n                    same_ua = false\n                    user_agent_list = []\n                }\n            }\n\n            this.parsedData.groups = Object.keys(temp_groups).map(key => temp_groups[key])\n        }\n\n        /**\n         * Check if a URL is allowed for specified user agent\n         * @param {string} url - URL to check\n         * @param {string} userAgent - User agent to check rules for\n         * @return {boolean} - True if allowed, false if disallowed\n         */\n        isAllowed(url, userAgent) {\n            if (typeof url === \"undefined\") throw new Error(\"The 'url' parameter is required.\")\n            if (typeof userAgent === \"undefined\") throw new Error(\"The 'userAgent' parameter is required.\")\n\n            /** @type {Rule[]} */\n            const rules = this.getApplicableRules(userAgent)\n            /** @type {string} */\n            const urlPath = this.normalizeUrlPath(url)\n            /** @type {Rule[]} */\n            const matchingRules = []\n\n            for (const rule of rules) {\n                if (rule.match(urlPath)) {\n                    matchingRules.push(rule)\n                }\n            }\n\n            if (matchingRules.length === 0) return true\n\n            /** @type {Rule} */\n            let mostSpecific = matchingRules[0]\n\n            // Find most specific rule based on path length and special characters\n            for (const rule of matchingRules) {\n                /** @type {number} */\n                const currentSpecificity = this.getRuleSpecificity(rule.path)\n                /** @type {number} */\n                const mostSpecificSpecificity = this.getRuleSpecificity(mostSpecific.path)\n\n                if (currentSpecificity > mostSpecificSpecificity) {\n                    mostSpecific = rule\n                }\n            }\n\n            return mostSpecific.type === \"allow\"\n        }\n\n        /**\n         * Check if a URL is disallowed for specified user agent\n         * @param {string} url - URL to check\n         * @param {string} userAgent - User agent to check rules for\n         * @return {boolean} - True if disallowed, false if allowed\n         */\n        isDisallowed(url, userAgent) {\n            return !this.isAllowed(url, userAgent)\n        }\n\n        /**\n         * Get sitemap URLs found in robots.txt\n         * @return {string[]} - Array of sitemap URLs\n         */\n        getSitemaps() {\n            return this.parsedData.sitemaps\n        }\n\n        /**\n         * Get group for specific user agent\n         * @param {string} userAgent - User agent to search for\n         * @return {Group|undefined} - Matching group or undefined\n         */\n        getGroup(userAgent) {\n            if (!userAgent) return undefined\n            for (let i = 0; i < this.parsedData.groups.length; i++) {\n                const group = this.parsedData.groups[i]\n\n                if (group.userAgent.toLowerCase() === userAgent.toLowerCase()) {\n                    return group\n                }\n            }\n            return undefined\n        }\n\n        /**\n         * Calculate rule specificity score for path comparison\n         * @private\n         * @param {string} path - URL path pattern\n         * @return {number} - Specificity score (higher = more specific)\n         */\n        getRuleSpecificity(path) {\n            /** @type {number} */\n            let specificity = path.length\n            if (path.indexOf(\"*\") !== -1) specificity -= 0.5\n            else if (path.slice(-1) === \"$\") specificity += 0.5\n            return specificity\n        }\n\n        /**\n         * Get groups applicable to specified user agent\n         * @private\n         * @param {string} userAgent - User agent to check\n         * @return {Group[]} - Array of matching groups\n         */\n        getApplicableGroups(userAgent) {\n            /** @type {Group[]} */\n            const exactGroups = this.parsedData.groups.filter(group => group.getName().toLowerCase() === userAgent.toLowerCase())\n            if (exactGroups.length > 0) return exactGroups\n            return this.parsedData.groups.filter(group => group.getName() === \"*\")\n        }\n\n        /**\n         * Get all rules applicable to specified user agent\n         * @private\n         * @param {string} userAgent - User agent to check\n         * @return {Rule[]} - Array of applicable rules\n         */\n        getApplicableRules(userAgent) {\n            /** @type {Rule[]} */\n            const rules = this.getApplicableGroups(userAgent)\n            return rules.reduce((acc, group) => acc.concat(group.getRules()), [])\n        }\n\n        /**\n         * Normalize URL path for comparison\n         * @private\n         * @param {string} url - URL or path to normalize\n         * @return {string} - Normalized path\n         */\n        normalizeUrlPath(url) {\n            try {\n                return this.normalizePath(new URL(url).pathname)\n            } catch (error) {\n                return this.normalizePath(url)\n            }\n        }\n\n        /**\n         * Normalize path string for consistent comparisons\n         * @private\n         * @param {string} path - URL path to normalize\n         * @return {string} - Normalized path\n         */\n        normalizePath(path) {\n            /** @type {string} */\n            let decodedPath\n            try {\n                decodedPath = decodeURIComponent(path)\n            } catch (error) {\n                decodedPath = path\n            }\n            /** @type {string} */\n            const newPath = decodedPath.replace(/\\/+/gu, \"/\")\n            if (newPath[0] === \"/\") return newPath\n            return `/${newPath}`\n        }\n    }\n\n    /**\n     * Create a new robots.txt parser instance\n     * @param {string} content - Raw robots.txt content\n     * @return {RobotsTxtParser} - Configured parser instance\n     */\n    function robotstxt(content) {\n        return new RobotsTxtParser(content)\n    }\n\n    // Universal module exports\n    /* eslint-disable quote-props */\n    if (typeof exports !== \"undefined\") {\n        exports.robotstxt = robotstxt\n    }\n    if (typeof window !== \"undefined\") {\n        window.robotstxt = robotstxt\n        if (typeof define === \"function\" && define.amd) {\n            define(() => ({ robotstxt }))\n        }\n    }\n    /* eslint-enable quote-props */\n}()\n"]}