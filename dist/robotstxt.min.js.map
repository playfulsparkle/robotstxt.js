{"version":3,"sources":["robotstxt.js"],"names":["Rule","constructor","type","path","this","Group","userAgent","crawlDelay","rules","getName","getCrawlDelay","getRules","allow","addRule","disallow","push","RobotsTxtParser","content","parsedData","groups","sitemaps","parse","line","new_content","split","let","processedLine","trim","directive","commentIndex","indexOf","slice","colonIndex","toLowerCase","value","unshift","user_agent_list","same_ua","index","length","current","next","forEach","agent","parseFloat","isNaN","Object","values","isAllowed","url","rule","getApplicableRules","urlPath","normalizeUrlPath","matchingRules","pathMatches","mostSpecific","currentSpecificity","getRuleSpecificity","isDisallowed","getSitemaps","getUserAgent","find","g","specificity","includes","endsWith","getApplicableGroups","exactGroups","filter","flatMap","normalizePath","URL","pathname","rulePath","regexPattern","replace","RegExp","test","singleSlash","decodeURIComponent","startsWith","robotstxt","exports","window","define","amd"],"mappings":";CAEC,WAMSA,EAMFC,YAAYC,EAAMC,GAEdC,KAAKF,KAAOA,EAEZE,KAAKD,KAAOA,CAChB,CACJ,OAKME,EAKFJ,YAAYK,GAERF,KAAKE,UAAYA,EAEjBF,KAAKG,WAAa,KAElBH,KAAKI,MAAQ,EACjB,CAMAC,UACI,OAAOL,KAAKE,SAChB,CAMAI,gBACI,OAAON,KAAKG,UAChB,CAMAI,WACI,OAAOP,KAAKI,KAChB,CAMAI,MAAMT,GACFC,KAAKS,QAAQ,QAASV,CAAI,CAC9B,CAMAW,SAASX,GACLC,KAAKS,QAAQ,WAAYV,CAAI,CACjC,CAOAU,QAAQX,EAAMC,GACVC,KAAKI,MAAMO,KAAK,IAAIf,EAAKE,EAAMC,CAAI,CAAC,CACxC,CACJ,OAKMa,EAKFf,YAAYgB,GAKRb,KAAKc,WAAa,CACdC,OAAQ,GACRC,SAAU,EACd,EAEAhB,KAAKiB,MAAMJ,CAAO,CACtB,CAOAI,MAAMJ,GACF,IAGWK,EAHLC,EAAc,GAGpB,IAAWD,KAAQL,EAAQO,MAAM,IAAI,EAAG,CACpCC,IAAIC,EAAgBJ,EAAKK,KAAK,EAC9B,IAWMC,EAXAC,EAAeH,EAAcI,QAAQ,GAAG,GAG1CJ,EADiB,CAAC,IAAlBG,EACgBH,EAAcK,MAAM,EAAGF,CAAY,EAAEF,KAAK,EAGzDD,IAGc,CAAC,KADdM,EAAaN,EAAcI,QAAQ,GAAG,KAGtCF,EAAYF,EAAcK,MAAM,EAAGC,CAAU,EAAEL,KAAK,EAAEM,YAAY,EAClEC,EAAQR,EAAcK,MAAMC,EAAa,CAAC,EAAEL,KAAK,EAEnDC,IAAaM,GACbX,EAAYR,KAAK,CAAEa,UAAAA,EAAWM,MAAAA,CAAM,CAAC,CAE7C,CAGIX,EAAY,IAAmC,eAA7BA,EAAY,GAAGK,WACjCL,EAAYY,QAAQ,CAAEP,UAAW,aAAcM,MAAO,GAAI,CAAC,EAG/DT,IAAIW,EAAkB,GAClBC,EAAU,CAAA,EAERlB,EAAS,GAGf,IAAKM,IAAIa,EAAQ,EAAGA,EAAQf,EAAYgB,OAAQD,CAAK,GAAI,CACrD,IAAME,EAAUjB,EAAYe,GAC5B,IAAMG,EAAOlB,EAAYe,EAAQ,GAEjC,GAA0B,eAAtBE,EAAQZ,UACRQ,EAAgBrB,KAAKyB,EAAQN,KAAK,EAC7Bf,EAAOqB,EAAQN,SAChBf,EAAOqB,EAAQN,OAAS,IAAI7B,EAAMmC,EAAQN,KAAK,QAGlD,GAA0B,UAAtBM,EAAQZ,UACbQ,EAAgBM,QAAQC,GAASxB,EAAOwB,GAAO/B,MAAM4B,EAAQN,KAAK,CAAC,EACnEG,EAAU,CAAA,OAET,GAA0B,aAAtBG,EAAQZ,UACbQ,EAAgBM,QAAQC,GAASxB,EAAOwB,GAAO7B,SAAS0B,EAAQN,KAAK,CAAC,EACtEG,EAAU,CAAA,OAET,GAA0B,gBAAtBG,EAAQZ,UAA6B,CAC1C,IAAMrB,EAAaqC,WAAWJ,EAAQN,KAAK,EACtCW,MAAMtC,CAAU,GACjB6B,EAAgBM,QAAQC,IACfxB,EAAOwB,GAAOpC,aACfY,EAAOwB,GAAOpC,WAAaA,EAEnC,CAAC,EAEL8B,EAAU,CAAA,CACd,KAC+B,YAAtBG,EAAQZ,WACbxB,KAAKc,WAAWE,SAASL,KAAKyB,EAAQN,KAAK,EAI3CO,GAAQJ,GAA8B,eAAnBI,EAAKb,YACxBS,EAAU,CAAA,EACVD,EAAkB,GAE1B,CAEAhC,KAAKc,WAAWC,OAAS2B,OAAOC,OAAO5B,CAAM,CACjD,CAQA6B,UAAUC,EAAK3C,EAAY,KACvB,IAIW4C,EAUAA,EAdL1C,EAAQJ,KAAK+C,mBAAmB7C,CAAS,EACzC8C,EAAUhD,KAAKiD,iBAAiBJ,CAAG,EACnCK,EAAgB,GAEtB,IAAWJ,KAAQ1C,EACXJ,KAAKmD,YAAYL,EAAK/C,KAAMiD,CAAO,GACnCE,EAAcvC,KAAKmC,CAAI,EAI/B,GAA6B,IAAzBI,EAAcf,OAAc,MAAO,CAAA,EAGvCd,IAAI+B,EAAeF,EAAc,GACjC,IAAWJ,KAAQI,EAAe,CAC9B,IAAMG,EAAqBrD,KAAKsD,mBAAmBR,EAAK/C,IAAI,EAC5BC,KAAKsD,mBAAmBF,EAAarD,IAAI,EAErEsD,IACAD,EAAeN,EAEvB,CAEA,MAA6B,UAAtBM,EAAatD,IACxB,CAQAyD,aAAaV,EAAK3C,EAAY,KAC1B,MAAO,CAACF,KAAK4C,UAAUC,EAAK3C,CAAS,CACzC,CAMAsD,cACI,OAAOxD,KAAKc,WAAWE,QAC3B,CAQAyC,aAAavD,EAAY,KACrB,OAAOF,KAAKc,WAAWC,OAAO2C,KAC1BC,GAAKA,EAAEzD,UAAU2B,YAAY,IAAM3B,EAAU2B,YAAY,CAC7D,GAAK,IACT,CAQAyB,mBAAmBvD,GACfsB,IAAIuC,EAAc7D,EAAKoC,OAGvB,OAFIpC,EAAK8D,SAAS,GAAG,EAAGD,GAAe,GAC9B7D,EAAK+D,SAAS,GAAG,IAAGF,GAAe,IACrCA,CACX,CAQAG,oBAAoB7D,GAChB,IAAM8D,EAAchE,KAAKc,WAAWC,OAAOkD,OACvCN,GAAKA,EAAEzD,UAAU2B,YAAY,IAAM3B,EAAU2B,YAAY,CAC7D,EACA,OAA4B,EAArBmC,EAAY7B,OAAa6B,EAC5BhE,KAAKc,WAAWC,OAAOkD,OAAON,GAAqB,MAAhBA,EAAEzD,SAAiB,CAC9D,CAQA6C,mBAAmB7C,GACf,OAAOF,KAAK+D,oBAAoB7D,CAAS,EAAEgE,QAAQP,GAAKA,EAAEpD,SAAS,CAAC,CACxE,CAQA0C,iBAAiBJ,GACb,IACI,OAAO7C,KAAKmE,cAAc,IAAIC,IAAIvB,CAAG,EAAEwB,QAAQ,CAGnD,CAFE,MACE,OAAOrE,KAAKmE,cAActB,CAAG,CACjC,CACJ,CASAM,YAAYmB,EAAUtB,GAEduB,EADmBvE,KAAKmE,cAAcG,CAAQ,EAE7CE,QAAQ,qBAAsB,MAAM,EACpCA,QAAQ,OAAQ,IAAI,EAEzB,OAAO,IAAIC,OAAO,IAAIF,EAAgB,GAAG,EAAEG,KAAK1B,CAAO,CAC3D,CAQAmB,cAAcpE,GAEJ4E,EADUC,mBAAmB7E,CAAI,EACXyE,QAAQ,OAAQ,GAAG,EAC/C,OAAOG,EAAYE,WAAW,GAAG,EAAIF,EAAc,IAAIA,CAC3D,CACJ,CAOA,SAASG,EAAUjE,GACf,OAAO,IAAID,EAAgBC,CAAO,CACtC,CAIuB,aAAnB,OAAOkE,UACPA,QAAQD,UAAYA,GAEF,aAAlB,OAAOE,SACPA,OAAOF,UAAYA,EACG,YAAlB,OAAOG,SAAyBA,OAAOC,KACvCD,OAAO,KAAM,CAAGH,UAAAA,CAAW,EAAC,CAIxC,GAAE","file":"robotstxt.min.js","sourcesContent":["/* global window, exports, define */\n\n!function () {\n    \"use strict\"\n\n    /**\n     * Single robots.txt rule (allow/disallow directive)\n     */\n    class Rule {\n        /**\n         * Create a new rule instance\n         * @param {string} type - Rule type ('allow' or 'disallow')\n         * @param {string} path - URL path pattern the rule applies to\n         */\n        constructor(type, path) {\n            /** @member {string} */\n            this.type = type\n            /** @member {string} */\n            this.path = path\n        }\n    }\n\n    /**\n     * Group of rules for a specific user agent\n     */\n    class Group {\n        /**\n         * Create a new user agent group\n         * @param {string} userAgent - User agent string this group applies to\n         */\n        constructor(userAgent) {\n            /** @member {string} */\n            this.userAgent = userAgent\n            /** @member {number|null} */\n            this.crawlDelay = null\n            /** @member {Rule[]} */\n            this.rules = []\n        }\n\n        /**\n         * Get the user agent name for this group\n         * @return {string}\n         */\n        getName() {\n            return this.userAgent\n        }\n\n        /**\n         * Get crawl delay setting for this group\n         * @return {number|null}\n         */\n        getCrawlDelay() {\n            return this.crawlDelay\n        }\n\n        /**\n         * Get all rules for this group\n         * @return {Rule[]}\n         */\n        getRules() {\n            return this.rules\n        }\n\n        /**\n         * Add an allow rule to the group\n         * @param {string} path - URL path pattern to allow\n         */\n        allow(path) {\n            this.addRule(\"allow\", path)\n        }\n\n        /**\n         * Add a disallow rule to the group\n         * @param {string} path - URL path pattern to disallow\n         */\n        disallow(path) {\n            this.addRule(\"disallow\", path)\n        }\n\n        /**\n         * Internal method to add a rule\n         * @param {string} type - Rule type ('allow' or 'disallow')\n         * @param {string} path - URL path pattern\n         */\n        addRule(type, path) {\n            this.rules.push(new Rule(type, path))\n        }\n    }\n\n    /**\n     * The robots.txt parser class\n     */\n    class RobotsTxtParser {\n        /**\n         * Create a new robots.txt parser\n         * @param {string} content - Raw robots.txt content to parse\n         */\n        constructor(content) {\n            /**\n             * @private\n             * @member {Object}\n             */\n            this.parsedData = {\n                groups: [],\n                sitemaps: []\n            }\n\n            this.parse(content)\n        }\n\n        /**\n         * Parse raw robots.txt content into structured format\n         * @private\n         * @param {string} content - Raw robots.txt content\n         */\n        parse(content) {\n            const new_content = []\n\n            // Preprocess lines: trim, remove comments, and split directives\n            for (const line of content.split(\"\\n\")) {\n                let processedLine = line.trim()\n                const commentIndex = processedLine.indexOf(\"#\")\n\n                if (commentIndex !== -1) {\n                    processedLine = processedLine.slice(0, commentIndex).trim()\n                }\n\n                if (!processedLine) continue\n\n                const colonIndex = processedLine.indexOf(\":\")\n                if (colonIndex === -1) continue\n\n                const directive = processedLine.slice(0, colonIndex).trim().toLowerCase()\n                const value = processedLine.slice(colonIndex + 1).trim()\n\n                if (directive && value) {\n                    new_content.push({ directive, value })\n                }\n            }\n\n            // Handle missing initial User-Agent\n            if (new_content[0] && new_content[0].directive !== \"user-agent\") {\n                new_content.unshift({ directive: \"user-agent\", value: \"*\" })\n            }\n\n            let user_agent_list = []\n            let same_ua = false\n            /** @type {Object.<string, Group>} */\n            const groups = {}\n\n            // Process each directive and build rule groups\n            for (let index = 0; index < new_content.length; index++) {\n                const current = new_content[index]\n                const next = new_content[index + 1]\n\n                if (current.directive === \"user-agent\") {\n                    user_agent_list.push(current.value)\n                    if (!groups[current.value]) {\n                        groups[current.value] = new Group(current.value)\n                    }\n                }\n                else if (current.directive === \"allow\") {\n                    user_agent_list.forEach(agent => groups[agent].allow(current.value))\n                    same_ua = true\n                }\n                else if (current.directive === \"disallow\") {\n                    user_agent_list.forEach(agent => groups[agent].disallow(current.value))\n                    same_ua = true\n                }\n                else if (current.directive === \"crawl-delay\") {\n                    const crawlDelay = parseFloat(current.value)\n                    if (!isNaN(crawlDelay)) {\n                        user_agent_list.forEach(agent => {\n                            if (!groups[agent].crawlDelay) {\n                                groups[agent].crawlDelay = crawlDelay\n                            }\n                        })\n                    }\n                    same_ua = true\n                }\n                else if (current.directive === \"sitemap\") {\n                    this.parsedData.sitemaps.push(current.value)\n                }\n\n                // Reset user agent list on new group\n                if (next && same_ua && next.directive === \"user-agent\") {\n                    same_ua = false\n                    user_agent_list = []\n                }\n            }\n\n            this.parsedData.groups = Object.values(groups)\n        }\n\n        /**\n         * Check if a URL is allowed for specified user agent\n         * @param {string} url - URL to check\n         * @param {string} [userAgent=\"*\"] - User agent to check rules for\n         * @return {boolean} - True if allowed, false if disallowed\n         */\n        isAllowed(url, userAgent = \"*\") {\n            const rules = this.getApplicableRules(userAgent)\n            const urlPath = this.normalizeUrlPath(url)\n            const matchingRules = []\n\n            for (const rule of rules) {\n                if (this.pathMatches(rule.path, urlPath)) {\n                    matchingRules.push(rule)\n                }\n            }\n\n            if (matchingRules.length === 0) return true\n\n            // Find most specific rule based on path length and special characters\n            let mostSpecific = matchingRules[0]\n            for (const rule of matchingRules) {\n                const currentSpecificity = this.getRuleSpecificity(rule.path)\n                const mostSpecificSpecificity = this.getRuleSpecificity(mostSpecific.path)\n\n                if (currentSpecificity > mostSpecificSpecificity) {\n                    mostSpecific = rule\n                }\n            }\n\n            return mostSpecific.type === \"allow\"\n        }\n\n        /**\n         * Check if a URL is disallowed for specified user agent\n         * @param {string} url - URL to check\n         * @param {string} [userAgent=\"*\"] - User agent to check rules for\n         * @return {boolean} - True if disallowed, false if allowed\n         */\n        isDisallowed(url, userAgent = \"*\") {\n            return !this.isAllowed(url, userAgent)\n        }\n\n        /**\n         * Get sitemap URLs found in robots.txt\n         * @return {string[]} - Array of sitemap URLs\n         */\n        getSitemaps() {\n            return this.parsedData.sitemaps\n        }\n\n        /**\n         * Get group for specific user agent\n         * @private\n         * @param {string} userAgent - User agent to search for\n         * @return {Group|null} - Matching group or null\n         */\n        getUserAgent(userAgent = \"*\") {\n            return this.parsedData.groups.find(\n                g => g.userAgent.toLowerCase() === userAgent.toLowerCase()\n            ) || null\n        }\n\n        /**\n         * Calculate rule specificity score for path comparison\n         * @private\n         * @param {string} path - URL path pattern\n         * @return {number} - Specificity score (higher = more specific)\n         */\n        getRuleSpecificity(path) {\n            let specificity = path.length\n            if (path.includes(\"*\")) specificity -= 0.5\n            else if (path.endsWith(\"$\")) specificity += 0.5\n            return specificity\n        }\n\n        /**\n         * Get groups applicable to specified user agent\n         * @private\n         * @param {string} userAgent - User agent to check\n         * @return {Group[]} - Array of matching groups\n         */\n        getApplicableGroups(userAgent) {\n            const exactGroups = this.parsedData.groups.filter(\n                g => g.userAgent.toLowerCase() === userAgent.toLowerCase()\n            )\n            return exactGroups.length > 0 ? exactGroups :\n                this.parsedData.groups.filter(g => g.userAgent === \"*\")\n        }\n\n        /**\n         * Get all rules applicable to specified user agent\n         * @private\n         * @param {string} userAgent - User agent to check\n         * @return {Rule[]} - Array of applicable rules\n         */\n        getApplicableRules(userAgent) {\n            return this.getApplicableGroups(userAgent).flatMap(g => g.getRules())\n        }\n\n        /**\n         * Normalize URL path for comparison\n         * @private\n         * @param {string} url - URL or path to normalize\n         * @return {string} - Normalized path\n         */\n        normalizeUrlPath(url) {\n            try {\n                return this.normalizePath(new URL(url).pathname)\n            } catch {\n                return this.normalizePath(url)\n            }\n        }\n\n        /**\n         * Check if URL path matches rule pattern\n         * @private\n         * @param {string} rulePath - Rule path pattern\n         * @param {string} urlPath - Normalized URL path\n         * @return {boolean} - True if path matches pattern\n         */\n        pathMatches(rulePath, urlPath) {\n            const normalizedRule = this.normalizePath(rulePath)\n            let regexPattern = normalizedRule\n                .replace(/[.^+?(){}[\\]|\\\\]/gu, \"\\\\$&\")\n                .replace(/\\*/gu, \".*\")\n\n            return new RegExp(`^${regexPattern}`, \"u\").test(urlPath)\n        }\n\n        /**\n         * Normalize path string for consistent comparisons\n         * @private\n         * @param {string} path - URL path to normalize\n         * @return {string} - Normalized path\n         */\n        normalizePath(path) {\n            const decoded = decodeURIComponent(path)\n            const singleSlash = decoded.replace(/\\/+/g, \"/\")\n            return singleSlash.startsWith(\"/\") ? singleSlash : `/${singleSlash}`\n        }\n    }\n\n    /**\n     * Create a new robots.txt parser instance\n     * @param {string} content - Raw robots.txt content\n     * @return {RobotsTxtParser} - Configured parser instance\n     */\n    function robotstxt(content) {\n        return new RobotsTxtParser(content)\n    }\n\n    // Universal module exports\n    /* eslint-disable quote-props */\n    if (typeof exports !== \"undefined\") {\n        exports.robotstxt = robotstxt\n    }\n    if (typeof window !== \"undefined\") {\n        window.robotstxt = robotstxt\n        if (typeof define === \"function\" && define.amd) {\n            define(() => ({ robotstxt }))\n        }\n    }\n    /* eslint-enable quote-props */\n}();\n"]}