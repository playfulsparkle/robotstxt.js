{"version":3,"sources":["robotstxt.js"],"names":["Rule","constructor","type","path","this","regex","createRegex","match","test","pattern","replace","RegExp","Group","userAgent","crawlDelay","undefined","cacheDelay","rules","comment","robotVersion","visitTime","requestRates","getName","getComment","getRobotVersion","getVisitTime","getRequestRates","getCacheDelay","getCrawlDelay","getRules","addRule","Error","push","RobotsTxtParser","content","groups","sitemaps","cleanParam","host","parse","line","normalizedContent","split","let","processedLine","trim","colonIndex","indexOf","directive","slice","toLowerCase","value","commentIndex","search","userAgentList","sameUserAgent","userAgentSeen","tempGroups","uaDirectives","index","length","currentLine","needsDefaultUa","uaName","normalizedPath","normalizePath","forEach","agent","isNaN","console","error","nextLine","Object","keys","map","key","isAllowed","url","rule","getApplicableRules","urlPath","normalizeUrlPath","matchingRules","mostSpecific","currentSpecificity","getRuleSpecificity","isDisallowed","getSitemaps","getCleanParams","getHost","getGroup","i","group","specificity","getApplicableGroups","exactGroups","filter","reduce","acc","concat","URL","pathname","decodedPath","decodeURIComponent","newPath","robotstxt","exports","window","define","amd"],"mappings":";CAEC,WAMSA,EAMFC,YAAYC,EAAMC,GAEdC,KAAKF,KAAOA,EAEZE,KAAKD,KAAOA,EAEZC,KAAKC,MAAQD,KAAKE,YAAYH,CAAI,CACtC,CAOAI,MAAMJ,GACF,OAAOC,KAAKC,MAAMG,KAAKL,CAAI,CAC/B,CAQAG,YAAYH,GACFM,EAAUN,EACXO,QAAQ,qBAAsB,MAAM,EACpCA,QAAQ,OAAQ,KAAK,EAE1B,OAAO,IAAIC,OAAO,IAAIF,EAAW,GAAG,CACxC,CACJ,OAKMG,EAKFX,YAAYY,GAERT,KAAKS,UAAYA,EAEjBT,KAAKU,WAAaC,KAAAA,EAElBX,KAAKY,WAAaD,KAAAA,EAElBX,KAAKa,MAAQ,GAEbb,KAAKc,QAAU,GAEfd,KAAKe,aAAeJ,KAAAA,EAEpBX,KAAKgB,UAAYL,KAAAA,EAEjBX,KAAKiB,aAAe,EACxB,CAMAC,UACI,OAAOlB,KAAKS,SAChB,CAMAU,aACI,OAAOnB,KAAKc,OAChB,CAMAM,kBACI,OAAOpB,KAAKe,YAChB,CAMAM,eACI,OAAOrB,KAAKgB,SAChB,CAMAM,kBACI,OAAOtB,KAAKiB,YAChB,CAMAM,gBACI,OAAOvB,KAAKY,UAChB,CAMAY,gBACI,OAAOxB,KAAKU,UAChB,CAMAe,WACI,OAAOzB,KAAKa,KAChB,CAOAa,QAAQ5B,EAAMC,GACV,GAAoB,KAAA,IAATD,EAAsB,MAAM,IAAI6B,MAAM,mCAAmC,EACpF,GAAoB,KAAA,IAAT5B,EAAsB,MAAM,IAAI4B,MAAM,mCAAmC,EACpF3B,KAAKa,MAAMe,KAAK,IAAIhC,EAAKE,EAAMC,CAAI,CAAC,CACxC,CACJ,OAKM8B,EAKFhC,YAAYiC,GAOR9B,KAAK+B,OAAS,GAQd/B,KAAKgC,SAAW,GAWhBhC,KAAKiC,WAAa,GAUlBjC,KAAKkC,KAAOvB,KAAAA,EAEZX,KAAKmC,MAAML,CAAO,CACtB,CAOAK,MAAML,GACF,GAAuB,KAAA,IAAZA,EAAyB,MAAM,IAAIH,MAAM,sCAAsC,EAG1F,IAEWS,EAFLC,EAAoB,GAE1B,IAAWD,KAAQN,EAAQQ,MAAM,YAAY,EAAG,CAE5CC,IAAIC,EAAgBJ,EAAKK,KAAK,EAE9B,GAAKD,GAAsC,MAArBA,EAAc,GAApC,CAGA,IAAME,EAAaF,EAAcG,QAAQ,GAAG,EAC5C,GAAmB,CAAC,IAAhBD,EAAJ,CAGA,IAAME,EAAYJ,EAAcK,MAAM,EAAGH,CAAU,EAAED,KAAK,EAAEK,YAAY,EAExEP,IAAIQ,EAAQP,EAAcK,MAAMH,EAAa,CAAC,EAAED,KAAK,EAG/CO,EAAeD,EAAME,OAAO,WAAW,EACxB,CAAC,IAAlBD,IACAD,EAAQA,EAAMF,MAAM,EAAGG,CAAY,EAAEP,KAAK,GAG1CG,GAAaG,GACbV,EAAkBT,KAAK,CAAEgB,UAAAA,EAAWG,MAAAA,CAAM,CAAC,CAdxB,CAJyB,CAoBpD,CAGAR,IAAIW,EAAgB,GAEhBC,EAAgB,CAAA,EAEhBC,EAAgB,CAAA,EAEdC,EAAa,GAGnB,IAAMC,EAAe,CACjB,QACA,WACA,UACA,UACA,gBACA,eACA,aACA,cACA,eAIJ,IAAKf,IAAIgB,EAAQ,EAAGA,EAAQlB,EAAkBmB,OAAQD,CAAK,GAAI,CAE3D,IAAME,EAAcpB,EAAkBkB,GAGtC,IAAMG,EAAiE,CAAC,IAAjDJ,EAAaX,QAAQc,EAAYb,SAAS,GAAY,CAACQ,EAgB9E,GAd8B,eAA1BK,EAAYb,WAA8Bc,CAAAA,IAC1CN,EAAgB,CAAA,EAEVO,EAASD,EAAiB,IAAMD,EAAYV,MAEX,CAAC,IAApC,CAACG,EAAcP,QAAQgB,CAAM,GAC7BT,EAActB,KAAK+B,CAAM,EAGxBN,EAAWM,MACZN,EAAWM,GAAU,IAAInD,EAAMmD,CAAM,GAIf,UAA1BF,EAAYb,UAAuB,CACnC,IAAMgB,EAAiB5D,KAAK6D,cAAcJ,EAAYV,KAAK,EAC3DG,EAAcY,QAAQC,GAASV,EAAWU,GAAOrC,QAAQ,QAASkC,CAAc,CAAC,EACjFT,EAAgB,CAAA,CACpB,MACK,GAA8B,aAA1BM,EAAYb,UAA0B,CAC3C,IAAMgB,EAAiB5D,KAAK6D,cAAcJ,EAAYV,KAAK,EAC3DG,EAAcY,QAAQC,GAASV,EAAWU,GAAOrC,QAAQ,WAAYkC,CAAc,CAAC,EACpFT,EAAgB,CAAA,CACpB,MACK,GAA8B,YAA1BM,EAAYb,UAAyB,CAC1C,IAAMgB,EAAiB5D,KAAK6D,cAAcJ,EAAYV,KAAK,EAC3DG,EAAcY,QAAQC,GAASV,EAAWU,GAAOrC,QAAQ,UAAWkC,CAAc,CAAC,EACnFT,EAAgB,CAAA,CACpB,MACK,GAA8B,gBAA1BM,EAAYb,UAA6B,CAC9C,IAAMhC,EAAa6C,CAAAA,EAAYV,MAE/B,GAAIiB,MAAMpD,CAAU,EAAG,CACnBqD,QAAQC,oCAAoCT,EAAYV,wBAAwB,EAChF,QACJ,CAAO,GAAInC,GAAc,EAAG,CACxBqD,QAAQC,sEAAsEtD,IAAa,EAC3F,QACJ,CAEAsC,EAAcY,QAAQC,IACbV,EAAWU,GAAOnD,aACnByC,EAAWU,GAAOnD,WAAaA,EAEvC,CAAC,EACDuC,EAAgB,CAAA,CACpB,MACK,GAA8B,gBAA1BM,EAAYb,UAA6B,CAC9C,IAAMlC,EAAa+C,CAAAA,EAAYV,MAE/B,GAAIiB,MAAMtD,CAAU,EAAG,CACnBuD,QAAQC,oCAAoCT,EAAYV,wBAAwB,EAChF,QACJ,CAAO,GAAIrC,GAAc,EAAG,CACxBuD,QAAQC,sEAAsExD,IAAa,EAC3F,QACJ,CAEAwC,EAAcY,QAAQC,IACbV,EAAWU,GAAOrD,aACnB2C,EAAWU,GAAOrD,WAAaA,EAEvC,CAAC,EACDyC,EAAgB,CAAA,CACpB,MACK,GAA8B,YAA1BM,EAAYb,UACjBM,EAAcY,QAAQC,GAASV,EAAWU,GAAOjD,QAAQc,KAAK6B,EAAYV,KAAK,CAAC,EAChFI,EAAgB,CAAA,OAEf,GAA8B,kBAA1BM,EAAYb,UAA+B,CAChD,GAAI,CAAC,oDAAoDxC,KAAKqD,EAAYV,KAAK,EAAG,CAC9EkB,QAAQC,sCAAsCT,EAAYV,kDAAkD,EAC5G,QACJ,CACAG,EAAcY,QAAQC,GAASV,EAAWU,GAAOhD,aAAe0C,EAAYV,KAAK,EACjFI,EAAgB,CAAA,CACpB,KACmC,iBAA1BM,EAAYb,WACjBM,EAAcY,QAAQC,GAASV,EAAWU,GAAO9C,aAAaW,KAAK6B,EAAYV,KAAK,CAAC,EACrFI,EAAgB,CAAA,GAEe,eAA1BM,EAAYb,WACjBM,EAAcY,QAAQC,GAASV,EAAWU,GAAO/C,UAAYyC,EAAYV,KAAK,EAC9EI,EAAgB,CAAA,GAEe,YAA1BM,EAAYb,UACjB5C,KAAKgC,SAASJ,KAAK6B,EAAYV,KAAK,EAEL,gBAA1BU,EAAYb,UACjB5C,KAAKiC,WAAWL,KAAK6B,EAAYV,KAAK,EAEP,SAA1BU,EAAYb,YACjB5C,KAAKkC,KAAOuB,EAAYV,OAItBoB,EAAW9B,EAAkBkB,EAAQ,GAGvCY,GAAYhB,GAAwC,eAAvBgB,EAASvB,YACtCO,EAAgB,CAAA,EAChBD,EAAgB,GAExB,CAEAlD,KAAK+B,OAASqC,OAAOC,KAAKhB,CAAU,EAAEiB,IAAIC,GAAOlB,EAAWkB,EAAI,CACpE,CAQAC,UAAUC,EAAKhE,GACX,GAAmB,KAAA,IAARgE,EAAqB,MAAM,IAAI9C,MAAM,kCAAkC,EAClF,GAAyB,KAAA,IAAdlB,EAA2B,MAAM,IAAIkB,MAAM,wCAAwC,EAG9F,IAMW+C,EAYAA,EAlBL7D,EAAQb,KAAK2E,mBAAmBlE,CAAS,EAEzCmE,EAAU5E,KAAK6E,iBAAiBJ,CAAG,EAEnCK,EAAgB,GAEtB,IAAWJ,KAAQ7D,EACX6D,EAAKvE,MAAMyE,CAAO,GAClBE,EAAclD,KAAK8C,CAAI,EAI/B,GAA6B,IAAzBI,EAActB,OAAc,MAAO,CAAA,EAGvCjB,IAAIwC,EAAeD,EAAc,GAGjC,IAAWJ,KAAQI,EAAe,CAE9B,IAAME,EAAqBhF,KAAKiF,mBAAmBP,EAAK3E,IAAI,EAE5BC,KAAKiF,mBAAmBF,EAAahF,IAAI,EAErEiF,IACAD,EAAeL,EAEvB,CAEA,MAA6B,UAAtBK,EAAajF,IACxB,CAQAoF,aAAaT,EAAKhE,GACd,MAAO,CAACT,KAAKwE,UAAUC,EAAKhE,CAAS,CACzC,CAMA0E,cACI,OAAOnF,KAAKgC,QAChB,CAQAoD,iBACI,OAAOpF,KAAKiC,UAChB,CASAoD,UACI,OAAOrF,KAAKkC,KAAOvB,KAAAA,CACvB,CAOA2E,SAAS7E,GACL,GAAKA,EACL,IAAK8B,IAAIgD,EAAI,EAAGA,EAAIvF,KAAK+B,OAAOyB,OAAQ+B,CAAC,GAAI,CACzC,IAAMC,EAAQxF,KAAK+B,OAAOwD,GAE1B,GAAIC,EAAM/E,UAAUqC,YAAY,IAAMrC,EAAUqC,YAAY,EACxD,OAAO0C,CAEf,CAEJ,CAQAP,mBAAmBlF,GAEfwC,IAAIkD,EAAc1F,EAAKyD,OAGvB,MAF0B,CAAC,IAAvBzD,EAAK4C,QAAQ,GAAG,EAAU8C,GAAe,GACjB,MAAnB1F,EAAK8C,MAAM,CAAC,CAAC,IAAW4C,GAAe,IACzCA,CACX,CAQAC,oBAAoBjF,GAEhB,IAAMkF,EAAc3F,KAAK+B,OAAO6D,OAAOJ,GAASA,EAAMtE,QAAQ,EAAE4B,YAAY,IAAMrC,EAAUqC,YAAY,CAAC,EACzG,OAAyB,EAArB6C,EAAYnC,OAAmBmC,EAC5B3F,KAAK+B,OAAO6D,OAAOJ,GAA6B,MAApBA,EAAMtE,QAAQ,CAAS,CAC9D,CAQAyD,mBAAmBlE,GAGf,OADcT,KAAK0F,oBAAoBjF,CAAS,EACnCoF,OAAO,CAACC,EAAKN,IAAUM,EAAIC,OAAOP,EAAM/D,SAAS,CAAC,EAAG,EAAE,CACxE,CAQAoD,iBAAiBJ,GACb,IACI,OAAOzE,KAAK6D,cAAc,IAAImC,IAAIvB,CAAG,EAAEwB,QAAQ,CAGnD,CAFE,MAAO/B,GACL,OAAOlE,KAAK6D,cAAcY,CAAG,CACjC,CACJ,CAQAZ,cAAc9D,GAEVwC,IAAI2D,EACJ,IACIA,EAAcC,mBAAmBpG,CAAI,CAGzC,CAFE,MAAOmE,GACLgC,EAAcnG,CAClB,CAEMqG,EAAUF,EAAY5F,QAAQ,QAAS,GAAG,EAChD,MAAmB,MAAf8F,EAAQ,GAAmBA,EACxB,IAAIA,CACf,CACJ,CAOA,SAASC,EAAUvE,GACf,OAAO,IAAID,EAAgBC,CAAO,CACtC,CAIuB,aAAnB,OAAOwE,UACPA,QAAQD,UAAYA,GAEF,aAAlB,OAAOE,SACPA,OAAOF,UAAYA,EACG,YAAlB,OAAOG,SAAyBA,OAAOC,KACvCD,OAAO,KAAM,CAAGH,UAAAA,CAAW,EAAC,CAIxC,GAAE","file":"robotstxt.min.js","sourcesContent":["/* global window, exports, define */\n\n!function () {\n    \"use strict\"\n\n    /**\n     * Single robots.txt rule (allow/disallow directive)\n     */\n    class Rule {\n        /**\n         * Create a new rule instance\n         * @param {string} type - Rule type ('allow', 'disallow' or 'noindex')\n         * @param {string} path - URL path pattern the rule applies to\n         */\n        constructor(type, path) {\n            /** @member {string} */\n            this.type = type\n            /** @member {string} */\n            this.path = path\n            /** @member {string} */\n            this.regex = this.createRegex(path)\n        }\n\n        /**\n         * Test if a normalized URL path matches this rule's pattern\n         * @param {string} path - Normalized URL path to test against\n         * @return {boolean} - True if the path matches the rule's pattern\n         */\n        match(path) {\n            return this.regex.test(path)\n        }\n\n        /**\n         * Convert robots.txt path pattern to regular expression\n         * @private\n         * @param {string} path - Normalized URL path pattern to convert\n         * @return {RegExp} - Regular expression for path matching\n         */\n        createRegex(path) {\n            const pattern = path\n                .replace(/[.^+?(){}[\\]|\\\\]/gu, \"\\\\$&\")  // Escape regex special characters\n                .replace(/\\*/gu, \".*?\")                // Replace * with non-greedy wildcard\n\n            return new RegExp(`^${pattern}`, \"u\")\n        }\n    }\n\n    /**\n     * Group of rules for a specific user agent\n     */\n    class Group {\n        /**\n         * Create a new user agent group\n         * @param {string} userAgent - User agent string this group applies to\n         */\n        constructor(userAgent) {\n            /** @member {string} - User agent identifier for this group */\n            this.userAgent = userAgent\n            /** @member {number|undefined} - Delay between crawler requests in seconds */\n            this.crawlDelay = undefined\n            /** @member {number|undefined} - Specifies the minimum interval for a robot to wait after caching one page, before starting to cache another in seconds */\n            this.cacheDelay = undefined\n            /** @member {Rule[]} - Collection of rules for this user agent */\n            this.rules = []\n            /** @member {string} - Optional comment associated with the group */\n            this.comment = []\n            /** @member {string|undefined} - Version of robots.txt specification used */\n            this.robotVersion = undefined\n            /** @member {string|undefined} - Recommended visit time from robots.txt */\n            this.visitTime = undefined\n            /** @member {string[]} - Request rate limits for this user agent */\n            this.requestRates = []\n        }\n\n        /**\n         * Get the user agent name for this group\n         * @return {string} User agent identifier\n         */\n        getName() {\n            return this.userAgent\n        }\n\n        /**\n         * Get the comment associated with this group\n         * @return {string[]} Group comment if available\n         */\n        getComment() {\n            return this.comment\n        }\n\n        /**\n         * Get the robots.txt specification version\n         * @return {string|undefined} Version number of robots.txt specification\n         */\n        getRobotVersion() {\n            return this.robotVersion\n        }\n\n        /**\n         * Get the recommended visit time for crawler\n         * @return {string|undefined} Suggested crawl time window\n         */\n        getVisitTime() {\n            return this.visitTime\n        }\n\n        /**\n         * Get request rate limitations for this group\n         * @return {string[]} Array of request rate rules\n         */\n        getRequestRates() {\n            return this.requestRates\n        }\n\n        /**\n         * Get crawl delay setting for this group\n         * @return {number|undefined} Delay between requests in seconds\n         */\n        getCacheDelay() {\n            return this.cacheDelay\n        }\n\n        /**\n         * Get crawl delay setting for this group\n         * @return {number|undefined} Delay between requests in seconds\n         */\n        getCrawlDelay() {\n            return this.crawlDelay\n        }\n\n        /**\n         * Get all rules for this group\n         * @return {Rule[]} Array of rule objects\n         */\n        getRules() {\n            return this.rules\n        }\n\n        /**\n         * Internal method to add a rule\n         * @param {string} type - Rule type ('allow', 'disallow', 'noindex')\n         * @param {string} path - URL path pattern\n         */\n        addRule(type, path) {\n            if (typeof type === \"undefined\") throw new Error(\"The 'type' parameter is required.\")\n            if (typeof path === \"undefined\") throw new Error(\"The 'path' parameter is required.\")\n            this.rules.push(new Rule(type, path))\n        }\n    }\n\n    /**\n     * The robots.txt parser class\n     */\n    class RobotsTxtParser {\n        /**\n         * Create a new robots.txt parser\n         * @param {string} content - Raw robots.txt content to parse\n         */\n        constructor(content) {\n            /**\n             * @private\n             * @type {Group[]}\n             * @description Collection of user agent groups containing access rules.\n             *              Represents all parsed User-agent sections from robots.txt\n             */\n            this.groups = []\n\n            /**\n             * @private\n             * @type {string[]}\n             * @description Array of absolute URLs to sitemaps specified in robots.txt.\n             *              Collected from Sitemap directives across the entire file.\n             */\n            this.sitemaps = []\n\n            /**\n             * @private\n             * @type {string[]}\n             * @description Collection of Clean-param directive values specifying\n             *              dynamic parameters that should be ignored during URL\n             *              canonicalization. These typically include tracking\n             *              parameters, session IDs, or other URL-specific values\n             *              that don't affect content.\n             */\n            this.cleanParam = []\n\n            /**\n             * @private\n             * @type {string|undefined}\n             * @description Preferred canonical host declaration from Host directive, used to:\n            *                 - Specify the primary domain when multiple mirrors exist\n            *                 - Handle internationalization/country targeting (ccTLDs)\n            *                 - Enforce consistent domain (with/without www) for search engines\n             */\n            this.host = undefined\n\n            this.parse(content)\n        }\n\n        /**\n         * Parse raw robots.txt content into structured format\n         * @private\n         * @param {string} content - Raw robots.txt content\n         */\n        parse(content) {\n            if (typeof content === \"undefined\") throw new Error(\"The 'content' parameter is required.\")\n\n            /** @type {string[]} */\n            const normalizedContent = []\n\n            for (const line of content.split(/\\r\\n|\\r|\\n/)) {\n                /** @type {string} */\n                let processedLine = line.trim()\n\n                if (!processedLine || processedLine[0] === \"#\") continue\n\n                /** @type {number} */\n                const colonIndex = processedLine.indexOf(\":\")\n                if (colonIndex === -1) continue\n\n                /** @type {string} */\n                const directive = processedLine.slice(0, colonIndex).trim().toLowerCase()\n                /** @type {string} */\n                let value = processedLine.slice(colonIndex + 1).trim()\n\n                /** @type {number} */\n                const commentIndex = value.search(/(?:\\s|^)#/)\n                if (commentIndex !== -1) {\n                    value = value.slice(0, commentIndex).trim()\n                }\n\n                if (directive && value) {\n                    normalizedContent.push({ directive, value })\n                }\n            }\n\n            /** @type {string[]} */\n            let userAgentList = []\n            /** @type {boolean} */\n            let sameUserAgent = false\n            /** @type {boolean} */\n            let userAgentSeen = false\n            /** @type {Object.<string, Group>} */\n            const tempGroups = {}\n\n            /** @type {string} - Array of directives which require at least one User-Agent present. */\n            const uaDirectives = [\n                \"allow\",\n                \"disallow\",\n                \"noindex\",\n                \"comment\",\n                \"robot-version\",\n                \"request-rate\",\n                \"visit-time\",\n                \"cache-delay\",\n                \"crawl-delay\"\n            ]\n\n            // Process each directive and build rule groups\n            for (let index = 0; index < normalizedContent.length; index++) {\n                /** @type {Object.<string, string>} */\n                const currentLine = normalizedContent[index]\n\n                /** @type {boolean} */\n                const needsDefaultUa = uaDirectives.indexOf(currentLine.directive) !== -1 && !userAgentSeen\n\n                if (currentLine.directive === \"user-agent\" || needsDefaultUa) {\n                    userAgentSeen = true\n\n                    const uaName = needsDefaultUa ? \"*\" : currentLine.value\n\n                    if (!userAgentList.indexOf(uaName) !== -1) {\n                        userAgentList.push(uaName)\n                    }\n\n                    if (!tempGroups[uaName]) {\n                        tempGroups[uaName] = new Group(uaName)\n                    }\n                }\n\n                if (currentLine.directive === \"allow\") {\n                    const normalizedPath = this.normalizePath(currentLine.value)\n                    userAgentList.forEach(agent => tempGroups[agent].addRule(\"allow\", normalizedPath))\n                    sameUserAgent = true\n                }\n                else if (currentLine.directive === \"disallow\") {\n                    const normalizedPath = this.normalizePath(currentLine.value)\n                    userAgentList.forEach(agent => tempGroups[agent].addRule(\"disallow\", normalizedPath))\n                    sameUserAgent = true\n                }\n                else if (currentLine.directive === \"noindex\") {\n                    const normalizedPath = this.normalizePath(currentLine.value)\n                    userAgentList.forEach(agent => tempGroups[agent].addRule(\"noindex\", normalizedPath))\n                    sameUserAgent = true\n                }\n                else if (currentLine.directive === \"cache-delay\") {\n                    const cacheDelay = currentLine.value * 1\n\n                    if (isNaN(cacheDelay)) {\n                        console.error(`Invalid Cache-delay value: ${currentLine.value} is not a number.`)\n                        continue\n                    } else if (cacheDelay <= 0) {\n                        console.error(`Cache-delay must be a positive number. The provided value is ${cacheDelay}.`)\n                        continue\n                    }\n\n                    userAgentList.forEach(agent => {\n                        if (!tempGroups[agent].cacheDelay) {\n                            tempGroups[agent].cacheDelay = cacheDelay\n                        }\n                    })\n                    sameUserAgent = true\n                }\n                else if (currentLine.directive === \"crawl-delay\") {\n                    const crawlDelay = currentLine.value * 1\n\n                    if (isNaN(crawlDelay)) {\n                        console.error(`Invalid Crawl-Delay value: ${currentLine.value} is not a number.`)\n                        continue\n                    } else if (crawlDelay <= 0) {\n                        console.error(`Crawl-Delay must be a positive number. The provided value is ${crawlDelay}.`)\n                        continue\n                    }\n\n                    userAgentList.forEach(agent => {\n                        if (!tempGroups[agent].crawlDelay) {\n                            tempGroups[agent].crawlDelay = crawlDelay\n                        }\n                    })\n                    sameUserAgent = true\n                }\n                else if (currentLine.directive === \"comment\") {\n                    userAgentList.forEach(agent => tempGroups[agent].comment.push(currentLine.value))\n                    sameUserAgent = true\n                }\n                else if (currentLine.directive === \"robot-version\") {\n                    if (!/^(\\d+)\\.(\\d+)\\.(\\d+)(?:-([a-zA-Z]+(?:\\.\\d+)?))?$/u.test(currentLine.value)) {\n                        console.error(`Invalid Robot-Version value: ${currentLine.value} does not match semantic versioning format.`)\n                        continue\n                    }\n                    userAgentList.forEach(agent => tempGroups[agent].robotVersion = currentLine.value)\n                    sameUserAgent = true\n                }\n                else if (currentLine.directive === \"request-rate\") {\n                    userAgentList.forEach(agent => tempGroups[agent].requestRates.push(currentLine.value))\n                    sameUserAgent = true\n                }\n                else if (currentLine.directive === \"visit-time\") {\n                    userAgentList.forEach(agent => tempGroups[agent].visitTime = currentLine.value)\n                    sameUserAgent = true\n                }\n                else if (currentLine.directive === \"sitemap\") {\n                    this.sitemaps.push(currentLine.value)\n                }\n                else if (currentLine.directive === \"clean-param\") {\n                    this.cleanParam.push(currentLine.value)\n                }\n                else if (currentLine.directive === \"host\") {\n                    this.host = currentLine.value\n                }\n\n                /** @type {Object.<string, string>} */\n                const nextLine = normalizedContent[index + 1]\n\n                // Reset user agent list on new group\n                if (nextLine && sameUserAgent && nextLine.directive === \"user-agent\") {\n                    sameUserAgent = false\n                    userAgentList = []\n                }\n            }\n\n            this.groups = Object.keys(tempGroups).map(key => tempGroups[key])\n        }\n\n        /**\n         * Check if a URL is allowed for specified user agent\n         * @param {string} url - URL to check\n         * @param {string} userAgent - User agent to check rules for\n         * @return {boolean} - True if allowed, false if disallowed\n         */\n        isAllowed(url, userAgent) {\n            if (typeof url === \"undefined\") throw new Error(\"The 'url' parameter is required.\")\n            if (typeof userAgent === \"undefined\") throw new Error(\"The 'userAgent' parameter is required.\")\n\n            /** @type {Rule[]} */\n            const rules = this.getApplicableRules(userAgent)\n            /** @type {string} */\n            const urlPath = this.normalizeUrlPath(url)\n            /** @type {Rule[]} */\n            const matchingRules = []\n\n            for (const rule of rules) {\n                if (rule.match(urlPath)) {\n                    matchingRules.push(rule)\n                }\n            }\n\n            if (matchingRules.length === 0) return true\n\n            /** @type {Rule} */\n            let mostSpecific = matchingRules[0]\n\n            // Find most specific rule based on path length and special characters\n            for (const rule of matchingRules) {\n                /** @type {number} */\n                const currentSpecificity = this.getRuleSpecificity(rule.path)\n                /** @type {number} */\n                const mostSpecificSpecificity = this.getRuleSpecificity(mostSpecific.path)\n\n                if (currentSpecificity > mostSpecificSpecificity) {\n                    mostSpecific = rule\n                }\n            }\n\n            return mostSpecific.type === \"allow\"\n        }\n\n        /**\n         * Check if a URL is disallowed for specified user agent\n         * @param {string} url - URL to check\n         * @param {string} userAgent - User agent to check rules for\n         * @return {boolean} - True if disallowed, false if allowed\n         */\n        isDisallowed(url, userAgent) {\n            return !this.isAllowed(url, userAgent)\n        }\n\n        /**\n         * Get sitemap URLs found in robots.txt\n         * @return {string[]} - Array of sitemap URLs\n         */\n        getSitemaps() {\n            return this.sitemaps\n        }\n\n        /**\n         * Retrieve Clean-param directives for URL parameter sanitization\n         * @returns {string[]} Array of parameter patterns in Clean-param format:\n         *                         - Each entry follows \"param[&param2] [path-prefix]\" syntax\n         *                         - Path prefix is optional and specifies URL scope\n         */\n        getCleanParams() {\n            return this.cleanParam\n        }\n\n        /**\n         * Get canonical host declaration for domain normalization\n         * @returns {string|undefined} Preferred hostname in one of these formats:\n         *                        - Domain without protocol (e.g., \"www.example.com\")\n         *                        - Domain with port (e.g., \"example.com:8080\")\n         *                        - undefined if no Host directive declared\n         */\n        getHost() {\n            return this.host = undefined\n        }\n\n        /**\n         * Get group for specific user agent\n         * @param {string} userAgent - User agent to search for\n         * @return {Group|undefined} - Matching group or undefined\n         */\n        getGroup(userAgent) {\n            if (!userAgent) return undefined\n            for (let i = 0; i < this.groups.length; i++) {\n                const group = this.groups[i]\n\n                if (group.userAgent.toLowerCase() === userAgent.toLowerCase()) {\n                    return group\n                }\n            }\n            return undefined\n        }\n\n        /**\n         * Calculate rule specificity score for path comparison\n         * @private\n         * @param {string} path - URL path pattern\n         * @return {number} - Specificity score (higher = more specific)\n         */\n        getRuleSpecificity(path) {\n            /** @type {number} */\n            let specificity = path.length\n            if (path.indexOf(\"*\") !== -1) specificity -= 0.5\n            else if (path.slice(-1) === \"$\") specificity += 0.5\n            return specificity\n        }\n\n        /**\n         * Get groups applicable to specified user agent\n         * @private\n         * @param {string} userAgent - User agent to check\n         * @return {Group[]} - Array of matching groups\n         */\n        getApplicableGroups(userAgent) {\n            /** @type {Group[]} */\n            const exactGroups = this.groups.filter(group => group.getName().toLowerCase() === userAgent.toLowerCase())\n            if (exactGroups.length > 0) return exactGroups\n            return this.groups.filter(group => group.getName() === \"*\")\n        }\n\n        /**\n         * Get all rules applicable to specified user agent\n         * @private\n         * @param {string} userAgent - User agent to check\n         * @return {Rule[]} - Array of applicable rules\n         */\n        getApplicableRules(userAgent) {\n            /** @type {Rule[]} */\n            const rules = this.getApplicableGroups(userAgent)\n            return rules.reduce((acc, group) => acc.concat(group.getRules()), [])\n        }\n\n        /**\n         * Normalize URL path for comparison\n         * @private\n         * @param {string} url - URL or path to normalize\n         * @return {string} - Normalized path\n         */\n        normalizeUrlPath(url) {\n            try {\n                return this.normalizePath(new URL(url).pathname)\n            } catch (error) {\n                return this.normalizePath(url)\n            }\n        }\n\n        /**\n         * Normalize path string for consistent comparisons\n         * @private\n         * @param {string} path - URL path to normalize\n         * @return {string} - Normalized path\n         */\n        normalizePath(path) {\n            /** @type {string} */\n            let decodedPath\n            try {\n                decodedPath = decodeURIComponent(path)\n            } catch (error) {\n                decodedPath = path\n            }\n            /** @type {string} */\n            const newPath = decodedPath.replace(/\\/+/gu, \"/\")\n            if (newPath[0] === \"/\") return newPath\n            return `/${newPath}`\n        }\n    }\n\n    /**\n     * Create a new robots.txt parser instance\n     * @param {string} content - Raw robots.txt content\n     * @return {RobotsTxtParser} - Configured parser instance\n     */\n    function robotstxt(content) {\n        return new RobotsTxtParser(content)\n    }\n\n    // Universal module exports\n    /* eslint-disable quote-props */\n    if (typeof exports !== \"undefined\") {\n        exports.robotstxt = robotstxt\n    }\n    if (typeof window !== \"undefined\") {\n        window.robotstxt = robotstxt\n        if (typeof define === \"function\" && define.amd) {\n            define(() => ({ robotstxt }))\n        }\n    }\n    /* eslint-enable quote-props */\n}()\n"]}