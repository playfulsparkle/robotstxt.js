<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>
    <script src="../src/robotstxt.js"></script>
</head>

<body>
    <script>
        // Parse robots.txt content
        const robotsTxtContent = `
User-Agent: GoogleBot
Allow: /public
Disallow: /private
Crawl-Delay: 5
Sitemap: https://example.com/sitemap.xml
`;

        const parser = robotstxt(robotsTxtContent);

        // Check URL permissions
        console.log(parser.isAllowed("/public/data", "GoogleBot"));  // true
        console.log(parser.isDisallowed("/private/admin", "GoogleBot")); // true

        // Get specific user agent group
        const googleBotGroup = parser.getGroup("googlebot"); // Case-insensitive
        if (googleBotGroup) {
            console.log("Crawl Delay:", googleBotGroup.getCrawlDelay()); // 5
            console.log("Rules:", googleBotGroup.getRules().map(rule =>
                `${rule.type}: ${rule.path}`
            )); // ["allow: /public", "disallow: /private"]
        }

        // Get all sitemaps
        console.log("Sitemaps:", parser.getSitemaps()); // ["https://example.com/sitemap.xml"]

        // Check default rules (wildcard *)
        console.log(parser.isAllowed("/protected", "*")); // true (if no wildcard rules exist)
    </script>
</body>

</html>
